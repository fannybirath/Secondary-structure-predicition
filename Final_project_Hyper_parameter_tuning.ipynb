{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "cTJh65tmZjeI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBRlNTjnZaRN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data(train_data, test_data):\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    # We'll use integer encoding for the secondary structure: H = 0, E = 1, C = 2\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # Flatten the sequences and labels\n",
        "    train_sequences_flat = train_sequences.reshape(-1, train_sequences.shape[2])  # Shape: (number of sequences * max_seq_len, 20)\n",
        "    test_sequences_flat = test_sequences.reshape(-1, test_sequences.shape[2])    # Shape: (number of sequences * max_seq_len, 20)\n",
        "    train_labels_flat = train_labels.flatten()  # Shape: (number of sequences * max_seq_len,)\n",
        "    test_labels_flat = test_labels.flatten()    # Shape: (number of sequences * max_seq_len,)\n",
        "\n",
        "    # Create mask to filter out padded positions (-1)\n",
        "    train_mask = train_labels_flat != -1\n",
        "    test_mask = test_labels_flat != -1\n",
        "\n",
        "    # Apply the mask to filter out padding\n",
        "    X_train = train_sequences_flat[train_mask]\n",
        "    y_train = train_labels_flat[train_mask]\n",
        "    X_test = test_sequences_flat[test_mask]\n",
        "    y_test = test_labels_flat[test_mask]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "def tune_random_forest(train_file_path, test_file_path, report_file_path, predictions_file_path):\n",
        "    # Step 1: Load Data\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Step 2: Preprocess Data\n",
        "    X_train, y_train, X_test, y_test = preprocess_data(train_data, test_data)\n",
        "\n",
        "    # Step 3: Define Hyperparameter Grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [10, 20, 30, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Step 4: Initialize and Fit Grid Search\n",
        "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Step 5: Predict and Evaluate\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Get the best parameters\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Step 6: Save Report and Predictions\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Random Forest Best Parameters:\\n\")\n",
        "        for param, value in best_params.items():\n",
        "            f.write(f\"{param}: {value}\\n\")\n",
        "        f.write(f\"\\nTest Accuracy: {accuracy:.4f}\\n\")\n",
        "        f.write(\"\\nClassification Report:\\n\")\n",
        "        f.write(classification_rep)\n",
        "\n",
        "    pd.DataFrame(y_pred, columns=['Predictions']).to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")\n",
        "\n",
        "    return best_model\n"
      ],
      "metadata": {
        "id": "0eKL1EchZoJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "best_rf = tune_random_forest('/content/training_data__part2_clean.csv', '/content/test_data_part2_clean.csv', 'RF_tuning_report.txt', 'RF_pred_tuning.csv')"
      ],
      "metadata": {
        "id": "Lhdf3_DzaAnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "nklYnAdjZqSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "wfR6w5XZZrh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data_for_nn(train_data, test_data):\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = [[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']]\n",
        "    test_labels = [[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']]\n",
        "\n",
        "    # Pad the labels to match the sequence length\n",
        "    train_labels = pad_sequences(train_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences(test_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels (convert to categorical format)\n",
        "    train_labels_categorical = np.where(train_labels[..., None] == -1, 0, to_categorical(train_labels, num_classes=3))\n",
        "    test_labels_categorical = np.where(test_labels[..., None] == -1, 0, to_categorical(test_labels, num_classes=3))\n",
        "\n",
        "    return train_sequences, train_labels_categorical, test_sequences, test_labels_categorical"
      ],
      "metadata": {
        "id": "flDZm0PvZtBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def preprocess_data_for_nn(train_data, test_data):\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = [[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']]\n",
        "    test_labels = [[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']]\n",
        "\n",
        "    # Pad the labels to match the sequence length\n",
        "    train_labels = pad_sequences(train_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences(test_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels (convert to categorical format)\n",
        "    train_labels_categorical = np.where(train_labels[..., None] == -1, 0, to_categorical(train_labels, num_classes=3))\n",
        "    test_labels_categorical = np.where(test_labels[..., None] == -1, 0, to_categorical(test_labels, num_classes=3))\n",
        "\n",
        "    return train_sequences, train_labels_categorical, test_sequences, test_labels_categorical\n",
        "\n",
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "    # Adding the Input layer for consistency\n",
        "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Conv1D(filters=hp.Choice('filters', [32, 64, 128]),\n",
        "                     kernel_size=hp.Choice('kernel_size', [3, 5]),\n",
        "                     activation='relu',\n",
        "                     padding='same'))  # Ensures the output length matches the input length\n",
        "    model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.3, 0.5])))\n",
        "    model.add(TimeDistributed(Dense(hp.Choice('dense_units', [64, 128]), activation='relu')))\n",
        "    model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-4])),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def tune_cnn(train_file_path, test_file_path, report_file_path, predictions_file_path):\n",
        "    # Step 1: Load Data\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Step 2: Preprocess Data for Neural Network\n",
        "    global X_train, y_train, X_test, y_test\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "\n",
        "    # Step 3: Set Up Tuner\n",
        "    tuner = kt.Hyperband(build_cnn_model,\n",
        "                         objective='val_accuracy',\n",
        "                         max_epochs=10,\n",
        "                         directory='cnn_tuning',\n",
        "                         project_name='cnn_hyperparameter_tuning')\n",
        "\n",
        "    # Step 4: Tuning Search\n",
        "    tuner.search(X_train, y_train, validation_split=0.1, epochs=10,\n",
        "                 callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
        "\n",
        "    # Step 5: Evaluate the Best Model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    y_pred = np.argmax(best_model.predict(X_test), axis=-1)\n",
        "    y_true = np.argmax(y_test, axis=-1)\n",
        "\n",
        "    # Flatten predictions and true labels\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    y_true_flat = y_true.flatten()\n",
        "\n",
        "    # Step 6: Save Report and Predictions\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Best Hyperparameters:\\n\")\n",
        "        f.write(str(tuner.get_best_hyperparameters()[0].values))\n",
        "        f.write(\"\\nClassification Report:\\n\")\n",
        "        f.write(classification_report(y_true_flat, y_pred_flat, target_names=['H', 'E', 'C']))\n",
        "\n",
        "    pd.DataFrame(y_pred_flat, columns=['Predictions']).to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")\n",
        "\n",
        "    return best_model\n"
      ],
      "metadata": {
        "id": "_a0XQrBNZvpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN\n",
        "best_cnn = tune_cnn('/content/training_data__part2_clean.csv', '/content/test_data_part2_clean.csv', 'CNN_tuning_report.txt', 'CNN_pred_tuning.csv')"
      ],
      "metadata": {
        "id": "-RJC5QPjaE4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hybrid CNN/RNN"
      ],
      "metadata": {
        "id": "LSoBhGUxZzjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM, TimeDistributed\n",
        "\n",
        "def tune_hybrid_cnn_rnn(train_file_path, test_file_path, report_file_path, predictions_file_path):\n",
        "    # Step 1: Load Data\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Step 2: Preprocess Data for Neural Network\n",
        "    global X_train, y_train, X_test, y_test\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "\n",
        "    # Step 3: Define the Hybrid Model\n",
        "    def build_hybrid_model(hp):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
        "        model.add(Conv1D(filters=hp.Choice('filters', [32, 64, 128]),\n",
        "                         kernel_size=hp.Choice('kernel_size', [3, 5]),\n",
        "                         activation='relu',\n",
        "                         padding='same'))  # Ensures the output length matches the input length\n",
        "        model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.3, 0.5])))\n",
        "        model.add(LSTM(units=hp.Choice('lstm_units', [32, 64, 128]),\n",
        "                       return_sequences=True))  # No padding argument here\n",
        "        model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.3, 0.5])))\n",
        "        model.add(TimeDistributed(Dense(hp.Choice('dense_units', [64, 128]), activation='relu')))\n",
        "        model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "        model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-4])),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Step 4: Set Up Tuner\n",
        "    tuner = kt.Hyperband(build_hybrid_model,\n",
        "                         objective='val_accuracy',\n",
        "                         max_epochs=10,\n",
        "                         directory='hybrid_tuning',\n",
        "                         project_name='hybrid_hyperparameter_tuning')\n",
        "\n",
        "    # Step 5: Tuning Search\n",
        "    tuner.search(X_train, y_train, validation_split=0.1, epochs=10,\n",
        "                 callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
        "\n",
        "    # Step 6: Evaluate the Best Model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    y_pred = np.argmax(best_model.predict(X_test), axis=-1)\n",
        "    y_true = np.argmax(y_test, axis=-1)\n",
        "\n",
        "    # Flatten predictions and true labels\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    y_true_flat = y_true.flatten()\n",
        "\n",
        "    # Step 7: Save Report and Predictions\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Best Hyperparameters:\\n\")\n",
        "        f.write(str(tuner.get_best_hyperparameters()[0].values))\n",
        "        f.write(\"\\nClassification Report:\\n\")\n",
        "        f.write(classification_report(y_true_flat, y_pred_flat, target_names=['H', 'E', 'C']))\n",
        "\n",
        "    pd.DataFrame(y_pred_flat, columns=['Predictions']).to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")\n",
        "\n",
        "    return best_model\n"
      ],
      "metadata": {
        "id": "IjwbGfRvZ4kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid CNN-RNN\n",
        "best_hybrid = tune_hybrid_cnn_rnn('/content/training_data__part2_clean.csv', '/content/test_data_part2_clean.csv', 'Hybrid_tuning_report.txt', 'Hybrid_pred_tuning.csv')"
      ],
      "metadata": {
        "id": "R0IEtmREaJrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}