{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Check in on the data"
      ],
      "metadata": {
        "id": "1bfrR-toWSdx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXY0AiwYWHkJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (assuming it's in a CSV format, adjust as necessary)\n",
        "file_path = '/content/training_data_clean.csv'  # Update this path to the actual location\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Dataset Overview:\")\n",
        "print(data.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Get the distribution of the secondary structure labels\n",
        "print(\"\\nSecondary Structure Distribution:\")\n",
        "print(data['sst3'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "KF_94URHWx4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As A Function\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_and_evaluate_rf(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # Flatten the sequences and labels\n",
        "    train_sequences_flat = train_sequences.reshape(-1, train_sequences.shape[2])\n",
        "    test_sequences_flat = test_sequences.reshape(-1, test_sequences.shape[2])\n",
        "    train_labels_flat = train_labels.flatten()\n",
        "    test_labels_flat = test_labels.flatten()\n",
        "\n",
        "    # Create mask to filter out padded positions (-1)\n",
        "    train_mask = train_labels_flat != -1\n",
        "    test_mask = test_labels_flat != -1\n",
        "\n",
        "    # Apply the mask to filter out padding\n",
        "    train_sequences_flat = train_sequences_flat[train_mask]\n",
        "    train_labels_flat = train_labels_flat[train_mask]\n",
        "    test_sequences_flat = test_sequences_flat[test_mask]\n",
        "    test_labels_flat = test_labels_flat[test_mask]\n",
        "\n",
        "    # Train the Random Forest model with the original settings\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf_model.fit(train_sequences_flat, train_labels_flat)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = rf_model.predict(test_sequences_flat)\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_labels_flat, test_predictions, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"Random Forest Test Accuracy: {rf_model.score(test_sequences_flat, test_labels_flat):.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_labels_flat,\n",
        "        'Predicted_Label': test_predictions\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")"
      ],
      "metadata": {
        "id": "PZ8Zdy08WXH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_rf(\n",
        "    train_file_path='/content/training_data_clean.csv',\n",
        "    test_file_path='/content/test_data_clean.csv',\n",
        "    report_file_path='RF_Initial_report.txt',\n",
        "    predictions_file_path='RF_initial_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "_i5rF0BSW96E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "SATWvb-fXARE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As a function\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_and_evaluate_cnn(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    train_labels_categorical = to_categorical(train_labels, num_classes=3)\n",
        "    test_labels_categorical = to_categorical(test_labels, num_classes=3)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = Sequential([\n",
        "        Input(shape=(train_sequences.shape[1], train_sequences.shape[2])),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
        "        Dropout(0.2),\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "        Dropout(0.3),\n",
        "        TimeDistributed(Dense(128, activation='relu')),\n",
        "        TimeDistributed(Dense(3, activation='softmax')),\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_sequences,\n",
        "        train_labels_categorical,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = model.predict(test_sequences)\n",
        "    test_predictions_labels = np.argmax(test_predictions, axis=-1)\n",
        "    test_true_labels = np.argmax(test_labels_categorical, axis=-1)\n",
        "\n",
        "    # Flatten the arrays to create a single list of predictions and true labels\n",
        "    test_predictions_flat = test_predictions_labels.flatten()\n",
        "    test_true_labels_flat = test_true_labels.flatten()\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_true_labels_flat, test_predictions_flat, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"CNN Test Accuracy: {model.evaluate(test_sequences, test_labels_categorical, verbose=0)[1]:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_true_labels_flat,\n",
        "        'Predicted_Label': test_predictions_flat\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")\n"
      ],
      "metadata": {
        "id": "0BfE8mKHW_Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data_clean.csv',\n",
        "    test_file_path='/content/test_data_clean.csv',\n",
        "    report_file_path='CNN_initial_report',\n",
        "    predictions_file_path='CNN_initial_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "Pj8U-BJ2XZGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hybrid CNN/RNN"
      ],
      "metadata": {
        "id": "cbKD-noyXdsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As a function\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, LSTM, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def train_and_evaluate_hybrid_cnn_rnn(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # Mask out padding from labels by setting the mask value to 0 in the one-hot encoded output\n",
        "    train_labels_categorical = np.where(train_labels[..., None] == -1, 0, to_categorical(train_labels, num_classes=3))\n",
        "    test_labels_categorical = np.where(test_labels[..., None] == -1, 0, to_categorical(test_labels, num_classes=3))\n",
        "\n",
        "    # Define the Hybrid CNN/RNN architecture\n",
        "    model = Sequential([\n",
        "        Input(shape=(train_sequences.shape[1], train_sequences.shape[2])),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
        "        Dropout(0.2),\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "        Dropout(0.3),\n",
        "        LSTM(64, return_sequences=True),\n",
        "        Dropout(0.5),\n",
        "        TimeDistributed(Dense(128, activation='relu')),\n",
        "        TimeDistributed(Dense(3, activation='softmax')),\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_sequences,\n",
        "        train_labels_categorical,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = model.predict(test_sequences)\n",
        "    test_predictions_labels = np.argmax(test_predictions, axis=-1)\n",
        "    test_true_labels = np.argmax(test_labels_categorical, axis=-1)\n",
        "\n",
        "    # Flatten the arrays to create a single list of predictions and true labels\n",
        "    test_predictions_flat = test_predictions_labels.flatten()\n",
        "    test_true_labels_flat = test_true_labels.flatten()\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_true_labels_flat, test_predictions_flat, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"Hybrid CNN/RNN Test Accuracy: {model.evaluate(test_sequences, test_labels_categorical, verbose=0)[1]:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_true_labels_flat,\n",
        "        'Predicted_Label': test_predictions_flat\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")"
      ],
      "metadata": {
        "id": "9HW1-xrIXdGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_hybrid_cnn_rnn(\n",
        "    train_file_path='/content/training_data_clean.csv',\n",
        "    test_file_path='/content/test_data_clean.csv',\n",
        "    report_file_path='Hybrid_initial_report.txt',\n",
        "    predictions_file_path='Hybrid_initial_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "PntUUrI9YXPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Secondary testing"
      ],
      "metadata": {
        "id": "OlaXgbE6Yh9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "wMTiN3oKYmVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test 2a\n",
        "train_and_evaluate_rf(\n",
        "    train_file_path='/content/training_data__part2_clean.csv',\n",
        "    test_file_path='/content/test_data_part2.csv',\n",
        "    report_file_path='RF_part2a_report.txt',\n",
        "    predictions_file_path='RF_part2a_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "UCbgtIluYpiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "ggdV-8G6YtKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2a\n",
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data__part2_clean.csv',\n",
        "    test_file_path='/content/test_data_part2_clean.csv',\n",
        "    report_file_path='CNN_part2a_report.txt',\n",
        "    predictions_file_path='CNN_part2a_predictions.csv'\n",
        ")\n"
      ],
      "metadata": {
        "id": "4C0ExbDUYvKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hybrid CNN/RNN"
      ],
      "metadata": {
        "id": "ODYBJRFsYxXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test 2a\n",
        "train_and_evaluate_hybrid_cnn_rnn(\n",
        "    train_file_path='/content/training_data__part2_clean.csv',\n",
        "    test_file_path='/content/test_data_part2_clean.csv',\n",
        "    report_file_path='Hybrid_part2a_report.txt',\n",
        "    predictions_file_path='Hybrid_part2a_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "2Www41-8Y04H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}