{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#CNN Function with only one convolution layer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_and_evaluate_cnn_1layer(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    epochs,\n",
        "    batch_size,\n",
        "    filter1,\n",
        "    kernelsize,\n",
        "    filter2,\n",
        "    dropout1,\n",
        "    dropout2,\n",
        "    dist_lay,\n",
        "    dist_lay2\n",
        "\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    train_labels_categorical = to_categorical(train_labels, num_classes=3)\n",
        "    test_labels_categorical = to_categorical(test_labels, num_classes=3)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = Sequential([\n",
        "        Input(shape=(train_sequences.shape[1], train_sequences.shape[2])),\n",
        "        Conv1D(filters=filter1, kernel_size=kernelsize, activation='relu', padding='same'),\n",
        "        Dropout(dropout1),\n",
        "        TimeDistributed(Dense(dist_lay, activation='relu')),\n",
        "        TimeDistributed(Dense(dist_lay2, activation='softmax')),\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_sequences,\n",
        "        train_labels_categorical,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = model.predict(test_sequences)\n",
        "    test_predictions_labels = np.argmax(test_predictions, axis=-1)\n",
        "    test_true_labels = np.argmax(test_labels_categorical, axis=-1)\n",
        "\n",
        "    # Flatten the arrays to create a single list of predictions and true labels\n",
        "    test_predictions_flat = test_predictions_labels.flatten()\n",
        "    test_true_labels_flat = test_true_labels.flatten()\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_true_labels_flat, test_predictions_flat, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"CNN Test Accuracy: {model.evaluate(test_sequences, test_labels_categorical, verbose=0)[1]:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_true_labels_flat,\n",
        "        'Predicted_Label': test_predictions_flat\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")"
      ],
      "metadata": {
        "id": "QjSHEU2CmY8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the one 1 layer one on the first dataset with initial parameters\n",
        "train_and_evaluate_cnn_1layer(\n",
        "    train_file_path='/content/training_data_clean.csv', #Training dataset\n",
        "    test_file_path='/content/test_data_clean.csv',  #Test dataset\n",
        "    report_file_path='CNN_initial_1layer_report.txt', #Report file containing ex: test accuracy and F1 scores\n",
        "    predictions_file_path='CNN_initial_1layer_predictions.csv', #Predicted labels by the model\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=32,\n",
        "    kernelsize=3,\n",
        "    filter2 =64,\n",
        "    dropout1=0.2,\n",
        "    dropout2=0.3,\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giB_thfop7sV",
        "outputId": "9870a3c2-f00d-40b6-dc49-c59b8324c220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 73ms/step - accuracy: 0.6594 - loss: 0.9134 - val_accuracy: 0.7473 - val_loss: 0.5919\n",
            "Epoch 2/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.7444 - loss: 0.5814 - val_accuracy: 0.7603 - val_loss: 0.5469\n",
            "Epoch 3/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.7526 - loss: 0.5535 - val_accuracy: 0.7615 - val_loss: 0.5340\n",
            "Epoch 4/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.7613 - loss: 0.5369 - val_accuracy: 0.7664 - val_loss: 0.5255\n",
            "Epoch 5/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 28ms/step - accuracy: 0.7683 - loss: 0.5261 - val_accuracy: 0.7680 - val_loss: 0.5200\n",
            "Epoch 6/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 44ms/step - accuracy: 0.7732 - loss: 0.5128 - val_accuracy: 0.7729 - val_loss: 0.5137\n",
            "Epoch 7/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7737 - loss: 0.5077 - val_accuracy: 0.7725 - val_loss: 0.5094\n",
            "Epoch 8/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.7760 - loss: 0.5054 - val_accuracy: 0.7765 - val_loss: 0.5077\n",
            "Epoch 9/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.7763 - loss: 0.5049 - val_accuracy: 0.7768 - val_loss: 0.5023\n",
            "Epoch 10/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.7834 - loss: 0.4892 - val_accuracy: 0.7774 - val_loss: 0.5011\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
            "Classification report saved to CNN_initial_1layer_report\n",
            "Predictions saved to CNN_initial_1layer_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test 1 layer network on the second dataset with initial parameters\n",
        "train_and_evaluate_cnn_1layer(\n",
        "    train_file_path='/content/training_data__part2_clean.csv',  #Training dataset\n",
        "    test_file_path='/content/test_data_part2_clean.csv',   #Test dataset\n",
        "    report_file_path='CNN_2_1layer_report.txt', #Report file containing ex: test accuracy and F1 scores\n",
        "    predictions_file_path='CNN_2_1layer_predictions.csv', #Predicted labels by the model\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=32,\n",
        "    kernelsize=3,\n",
        "    filter2 =64,\n",
        "    dropout1=0.2,\n",
        "    dropout2=0.3,\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0Y2s25Qq3FG",
        "outputId": "6ae45826-61b3-4d48-a51e-dc79497f5cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.6801 - loss: 0.8227 - val_accuracy: 0.7724 - val_loss: 0.5282\n",
            "Epoch 2/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.7569 - loss: 0.5460 - val_accuracy: 0.7811 - val_loss: 0.5055\n",
            "Epoch 3/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7666 - loss: 0.5232 - val_accuracy: 0.7860 - val_loss: 0.4930\n",
            "Epoch 4/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.7720 - loss: 0.5114 - val_accuracy: 0.7887 - val_loss: 0.4854\n",
            "Epoch 5/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 26ms/step - accuracy: 0.7738 - loss: 0.5076 - val_accuracy: 0.7925 - val_loss: 0.4771\n",
            "Epoch 6/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.7753 - loss: 0.5012 - val_accuracy: 0.7962 - val_loss: 0.4730\n",
            "Epoch 7/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.7805 - loss: 0.4927 - val_accuracy: 0.7984 - val_loss: 0.4691\n",
            "Epoch 8/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step - accuracy: 0.7816 - loss: 0.4910 - val_accuracy: 0.7983 - val_loss: 0.4678\n",
            "Epoch 9/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 27ms/step - accuracy: 0.7835 - loss: 0.4875 - val_accuracy: 0.8017 - val_loss: 0.4637\n",
            "Epoch 10/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.7831 - loss: 0.4880 - val_accuracy: 0.8017 - val_loss: 0.4628\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step\n",
            "Classification report saved to CNN_2_1layer_report\n",
            "Predictions saved to CNN_2_1layer_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J08ePo6vjrX7"
      },
      "outputs": [],
      "source": [
        "#CNN function with 2 convolution layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_and_evaluate_cnn(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    epochs,\n",
        "    batch_size,\n",
        "    filter1,\n",
        "    kernelsize,\n",
        "    filter2,\n",
        "    dropout1,\n",
        "    dropout2,\n",
        "    dist_lay,\n",
        "    dist_lay2\n",
        "\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    train_labels_categorical = to_categorical(train_labels, num_classes=3)\n",
        "    test_labels_categorical = to_categorical(test_labels, num_classes=3)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = Sequential([\n",
        "        Input(shape=(train_sequences.shape[1], train_sequences.shape[2])),\n",
        "        Conv1D(filters=filter1, kernel_size=kernelsize, activation='relu', padding='same'),\n",
        "        Dropout(dropout1),\n",
        "        Conv1D(filters=filter2, kernel_size=kernelsize, activation='relu', padding='same'),\n",
        "        Dropout(dropout2),\n",
        "        TimeDistributed(Dense(dist_lay, activation='relu')),\n",
        "        TimeDistributed(Dense(dist_lay2, activation='softmax')),\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_sequences,\n",
        "        train_labels_categorical,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = model.predict(test_sequences)\n",
        "    test_predictions_labels = np.argmax(test_predictions, axis=-1)\n",
        "    test_true_labels = np.argmax(test_labels_categorical, axis=-1)\n",
        "\n",
        "    # Flatten the arrays to create a single list of predictions and true labels\n",
        "    test_predictions_flat = test_predictions_labels.flatten()\n",
        "    test_true_labels_flat = test_true_labels.flatten()\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_true_labels_flat, test_predictions_flat, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"CNN Test Accuracy: {model.evaluate(test_sequences, test_labels_categorical, verbose=0)[1]:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_true_labels_flat,\n",
        "        'Predicted_Label': test_predictions_flat\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data_clean.csv', #Training dataset\n",
        "    test_file_path='/content/test_data_clean.csv', #Test dataset\n",
        "    report_file_path='CNN_initial_report',#Report file containing ex: test accuracy and F1 scores\n",
        "    predictions_file_path='CNN_initial_predictions.csv',#Predicted labels by the model\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=32,\n",
        "    kernelsize=3,\n",
        "    filter2 =64,\n",
        "    dropout1=0.2,\n",
        "    dropout2=0.3,\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLGrfNsdkna3",
        "outputId": "f77a714d-08a2-4ec6-fb8e-91f7a0d3fe6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - accuracy: 0.6780 - loss: 0.8548 - val_accuracy: 0.7633 - val_loss: 0.5395\n",
            "Epoch 2/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.7768 - loss: 0.5227 - val_accuracy: 0.7757 - val_loss: 0.5009\n",
            "Epoch 3/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.7866 - loss: 0.4968 - val_accuracy: 0.7885 - val_loss: 0.4843\n",
            "Epoch 4/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.7944 - loss: 0.4828 - val_accuracy: 0.7936 - val_loss: 0.4729\n",
            "Epoch 5/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 77ms/step - accuracy: 0.8024 - loss: 0.4665 - val_accuracy: 0.8023 - val_loss: 0.4602\n",
            "Epoch 6/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - accuracy: 0.8040 - loss: 0.4612 - val_accuracy: 0.8073 - val_loss: 0.4506\n",
            "Epoch 7/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 64ms/step - accuracy: 0.8035 - loss: 0.4616 - val_accuracy: 0.8125 - val_loss: 0.4452\n",
            "Epoch 8/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.8100 - loss: 0.4499 - val_accuracy: 0.8103 - val_loss: 0.4414\n",
            "Epoch 9/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.8136 - loss: 0.4420 - val_accuracy: 0.8181 - val_loss: 0.4336\n",
            "Epoch 10/10\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 63ms/step - accuracy: 0.8176 - loss: 0.4321 - val_accuracy: 0.8194 - val_loss: 0.4285\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step\n",
            "Classification report saved to CNN_initial_report\n",
            "Predictions saved to CNN_initial_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the 2 layer network on the second data set with initial parameter values\n",
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data__part2_clean.csv', #Training dataset\n",
        "    test_file_path='/content/test_data_part2_clean.csv',  #Test dataset\n",
        "    report_file_path='CNN_2a_report', #Report file containing ex: test accuracy and F1 scores\n",
        "    predictions_file_path='CNN_2a_predictions.csv', #Predicted labels by the model\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=32,\n",
        "    kernelsize=3,\n",
        "    filter2 =64,\n",
        "    dropout1=0.2,\n",
        "    dropout2=0.3,\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAqaJvyLrRBC",
        "outputId": "487350cb-4c9b-46de-b160-939783d7df7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 41ms/step - accuracy: 0.7060 - loss: 0.7443 - val_accuracy: 0.7973 - val_loss: 0.4793\n",
            "Epoch 2/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 57ms/step - accuracy: 0.7852 - loss: 0.5001 - val_accuracy: 0.8117 - val_loss: 0.4486\n",
            "Epoch 3/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 62ms/step - accuracy: 0.7961 - loss: 0.4782 - val_accuracy: 0.8242 - val_loss: 0.4242\n",
            "Epoch 4/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.8042 - loss: 0.4593 - val_accuracy: 0.8320 - val_loss: 0.4113\n",
            "Epoch 5/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - accuracy: 0.8120 - loss: 0.4436 - val_accuracy: 0.8365 - val_loss: 0.4002\n",
            "Epoch 6/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 42ms/step - accuracy: 0.8176 - loss: 0.4339 - val_accuracy: 0.8395 - val_loss: 0.3939\n",
            "Epoch 7/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8205 - loss: 0.4251 - val_accuracy: 0.8428 - val_loss: 0.3896\n",
            "Epoch 8/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.8226 - loss: 0.4224 - val_accuracy: 0.8449 - val_loss: 0.3877\n",
            "Epoch 9/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 42ms/step - accuracy: 0.8242 - loss: 0.4158 - val_accuracy: 0.8462 - val_loss: 0.3789\n",
            "Epoch 10/10\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.8268 - loss: 0.4133 - val_accuracy: 0.8490 - val_loss: 0.3767\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
            "Classification report saved to CNN_2a_report\n",
            "Predictions saved to CNN_2a_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the tuner needed to optimize parameters\n",
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "jM5Qvv2JeP9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The tuning code needed to run\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def preprocess_data_for_nn(train_data, test_data):\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = [[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']]\n",
        "    test_labels = [[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']]\n",
        "\n",
        "    # Pad the labels to match the sequence length\n",
        "    train_labels = pad_sequences(train_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences(test_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels (convert to categorical format)\n",
        "    train_labels_categorical = np.where(train_labels[..., None] == -1, 0, to_categorical(train_labels, num_classes=3))\n",
        "    test_labels_categorical = np.where(test_labels[..., None] == -1, 0, to_categorical(test_labels, num_classes=3))\n",
        "\n",
        "    return train_sequences, train_labels_categorical, test_sequences, test_labels_categorical\n",
        "\n",
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "    # Adding the Input layer for consistency\n",
        "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Conv1D(filters=hp.Choice('filters', [32, 64, 128]),\n",
        "                     kernel_size=hp.Choice('kernel_size', [3, 5]),\n",
        "                     activation='relu',\n",
        "                     padding='same'))  # Ensures the output length matches the input length\n",
        "    model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.3, 0.5])))\n",
        "    model.add(TimeDistributed(Dense(hp.Choice('dense_units', [64, 128]), activation='relu')))\n",
        "    model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-4])),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def tune_cnn(train_file_path, test_file_path, report_file_path, predictions_file_path):\n",
        "    # Step 1: Load Data\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Step 2: Preprocess Data for Neural Network\n",
        "    global X_train, y_train, X_test, y_test\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "\n",
        "    # Step 3: Set Up Tuner\n",
        "    tuner = kt.Hyperband(build_cnn_model,\n",
        "                         objective='val_accuracy',\n",
        "                         max_epochs=10,\n",
        "                         directory='cnn_tuning',\n",
        "                         project_name='cnn_hyperparameter_tuning')\n",
        "\n",
        "    # Step 4: Tuning Search\n",
        "    tuner.search(X_train, y_train, validation_split=0.1, epochs=10,\n",
        "                 callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
        "\n",
        "    # Step 5: Evaluate the Best Model\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "    y_pred = np.argmax(best_model.predict(X_test), axis=-1)\n",
        "    y_true = np.argmax(y_test, axis=-1)\n",
        "\n",
        "    # Flatten predictions and true labels\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    y_true_flat = y_true.flatten()\n",
        "\n",
        "    # Step 6: Save Report and Predictions\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Best Hyperparameters:\\n\")\n",
        "        f.write(str(tuner.get_best_hyperparameters()[0].values))\n",
        "        f.write(\"\\nClassification Report:\\n\")\n",
        "        f.write(classification_report(y_true_flat, y_pred_flat, target_names=['H', 'E', 'C']))\n",
        "\n",
        "    pd.DataFrame(y_pred_flat, columns=['Predictions']).to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")\n",
        "\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "JTtSp_8sb9-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the tuning for the CNN (input is training data (Dataset2), Testdata (Dataset 2), names for the CNN fine tuning report consisting of the best parameter values, And the predicted labels done by the CNN)\n",
        "best_cnn = tune_cnn('/content/training_data__part2_clean.csv', '/content/test_data_part2_clean.csv', 'CNN_tuning_report.txt', 'CNN_pred_tuning.csv')"
      ],
      "metadata": {
        "id": "OWsxq_BVcTtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the 2 layer network on the second data set with finetuned parameters. remember to run train_and_evaluate_cnn code first\n",
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data__part2_clean.csv', #Training dataset\n",
        "    test_file_path='/content/test_data_part2_clean.csv',  #Test dataset\n",
        "    report_file_path='CNN_2a_report', #Report file containing ex: test accuracy and F1 scores\n",
        "    predictions_file_path='CNN_2a_predictions.csv', #Predicted labels by the model\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=128,\n",
        "    kernelsize=5,\n",
        "    filter2 =128,\n",
        "    dropout1=0.3,\n",
        "    dropout2=0.3,\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")"
      ],
      "metadata": {
        "id": "cEB5T_GecteP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the 2 layer network on the third data set with finetuned parameters\n",
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data__part3_clean.csv',\n",
        "    test_file_path='/content/test_data_part3_clean.csv',\n",
        "    report_file_path='CNN_opt_done_part3_report.txt',\n",
        "    predictions_file_path='CNN_opt_done_part3_predictions.csv',\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=128,\n",
        "    kernelsize=5,\n",
        "    filter2 =128,\n",
        "    dropout1=0.3,\n",
        "    dropout2=0.3,\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")"
      ],
      "metadata": {
        "id": "-4xz1MJ8dh7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the 2 layer network on the fourth data set with finetuned parameters\n",
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data_part4.csv',\n",
        "    test_file_path='/content/training_data_part4.csv',\n",
        "    report_file_path='CNN_part4_report.txt',\n",
        "    predictions_file_path='CNN__part4_predictions.csv',\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=128,\n",
        "    kernelsize=5,\n",
        "    filter2 =128,\n",
        "    dropout1=0.3,\n",
        "    dropout2=0.3,\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")"
      ],
      "metadata": {
        "id": "zBrt-tTldvf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Broad tuning function to do further optimization\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def broad_optimization(train_file_path, test_file_path, report_file_path):\n",
        "    # Load and preprocess the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "\n",
        "    # Get input shape based on training data\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Broad search space\n",
        "    def build_model(hp):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=input_shape))\n",
        "        model.add(Conv1D(filters=hp.Choice('filters', [32, 64, 128, 256]),\n",
        "                         kernel_size=hp.Choice('kernel_size', [3, 5, 7]),\n",
        "                         activation='relu',\n",
        "                         padding='same'))\n",
        "        model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.3, 0.5, 0.6])))\n",
        "        model.add(TimeDistributed(Dense(hp.Choice('dense_units', [64, 128, 256]), activation='relu')))\n",
        "        model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "\n",
        "\n",
        "        # Compile with variable learning rate\n",
        "        model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Broad tuning using Hyperband\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_epochs=20,\n",
        "        directory='broad_tuning',\n",
        "        project_name='cnn_broad_optimization'\n",
        "    )\n",
        "\n",
        "    # Perform search\n",
        "    tuner.search(X_train, y_train, validation_split=0.1, epochs=10, batch_size=32)\n",
        "\n",
        "    # Retrieve best hyperparameters\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Save best hyperparameters to a report file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Best Hyperparameters from Broad Search:\\n\")\n",
        "        for param, value in best_hps.values.items():\n",
        "            f.write(f\"{param}: {value}\\n\")\n",
        "\n",
        "    print(f\"Best hyperparameters from broad search saved to {report_file_path}\")\n",
        "\n",
        "    return best_hps\n"
      ],
      "metadata": {
        "id": "Yk7J7aereFOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "broad_hps = broad_optimization(\n",
        "    train_file_path='/content/training_data_part4.csv',\n",
        "    test_file_path='/content/test_data_part4.csv',\n",
        "    report_file_path='broad_optimization_report_CNN_part4.txt'\n",
        ")\n"
      ],
      "metadata": {
        "id": "RUk1k3YaedvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine tuning function based on the broad tuning\n",
        "def fine_tuning(train_file_path, test_file_path, broad_hps, report_file_path):\n",
        "    # Fine-tuning within narrower ranges based on broad search results\n",
        "    def build_fine_tune_model(hp):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=input_shape))\n",
        "        model.add(Conv1D(filters=hp.Int('filters', min(broad_hps['filters']-32, 96),\n",
        "                                         max(broad_hps['filters']+32, 160), step=32),\n",
        "                         kernel_size=hp.Choice('kernel_size', [5, 7, 9]),\n",
        "                         activation='relu',\n",
        "                         padding='same'))\n",
        "        model.add(Dropout(hp.Float('dropout_rate', max(0.2, broad_hps['dropout_rate']-0.1),\n",
        "                                    min(broad_hps['dropout_rate']+0.1, 0.4), step=0.05)))\n",
        "        model.add(TimeDistributed(Dense(hp.Int('dense_units', min(broad_hps['dense_units']-32, 96),\n",
        "                                               max(broad_hps['dense_units']+32, 160), step=32),\n",
        "                                      activation='relu')))\n",
        "        model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "\n",
        "        # Compile with refined learning rate\n",
        "        model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', max(1e-3, broad_hps['learning_rate']/2),\n",
        "                                                            min(broad_hps['learning_rate'], 1e-2))),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Fine-tuning with Bayesian Optimization\n",
        "    tuner = kt.BayesianOptimization(\n",
        "        build_fine_tune_model,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=20,\n",
        "        directory='fine_tuning',\n",
        "        project_name='cnn_fine_tuning'\n",
        "    )\n",
        "\n",
        "    # Perform fine-tuning search\n",
        "    tuner.search(X_train, y_train, validation_split=0.1, epochs=20, batch_size=32)\n",
        "\n",
        "    # Retrieve best hyperparameters\n",
        "    best_fine_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Save best hyperparameters from fine-tuning to a report file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Best Hyperparameters from Fine-Tuning:\\n\")\n",
        "        for param, value in best_fine_hps.values.items():\n",
        "            f.write(f\"{param}: {value}\\n\")\n",
        "\n",
        "    print(f\"Best hyperparameters from fine-tuning saved to {report_file_path}\")\n",
        "\n",
        "    return best_fine_hps"
      ],
      "metadata": {
        "id": "rW0PIyWneiCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform finetuning with the results from the broad tuning\n",
        "fine_hps = fine_tuning(\n",
        "    train_file_path='/content/training_data_part4.csv',\n",
        "    test_file_path='/content/test_data_part4.csv',\n",
        "    broad_hps={\n",
        "        'filters': 128,\n",
        "        'kernel_size': 7,\n",
        "        'dropout_rate': 0.3,\n",
        "        'dense_units': 128,\n",
        "        'learning_rate': 0.01\n",
        "    },\n",
        "    report_file_path='fine_tuning_report_part4.txt'\n",
        ")\n"
      ],
      "metadata": {
        "id": "mOgPZABleqnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tested the fine tuned parameters\n",
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data_part4.csv',\n",
        "    test_file_path='/content/test_data_part4.csv',\n",
        "    report_file_path='final_optimized_cnn_report.txt',\n",
        "    predictions_file_path='final_optimized_cnn_predictions.csv',\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    filter1=128,\n",
        "    kernelsize=7,             # Corrected from kernel_size to match function\n",
        "    filter2=128,\n",
        "    dropout1=0.3,             # Corrected from dropout_rate1 to match function\n",
        "    dropout2=0.3,             # Corrected from dropout_rate2 to match function\n",
        "    dist_lay=128,\n",
        "    dist_lay2=3\n",
        ")\n"
      ],
      "metadata": {
        "id": "6jpnhGdSexLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}