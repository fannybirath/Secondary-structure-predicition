{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CNN Testing"
      ],
      "metadata": {
        "id": "5WgxhGQAc82c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcHQPrxxcO1t"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_and_evaluate_cnn(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    filters=128,\n",
        "    kernel_size=5,\n",
        "    dropout_rate=0.3,\n",
        "    dense_units=128,\n",
        "    learning_rate=0.001,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    train_labels_categorical = to_categorical(train_labels, num_classes=3)\n",
        "    test_labels_categorical = to_categorical(test_labels, num_classes=3)\n",
        "\n",
        "    # Define the CNN architecture based on the provided parameters\n",
        "    model = Sequential([\n",
        "        Input(shape=(train_sequences.shape[1], train_sequences.shape[2])),\n",
        "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'),\n",
        "        Dropout(dropout_rate),\n",
        "        TimeDistributed(Dense(dense_units, activation='relu')),\n",
        "        TimeDistributed(Dense(3, activation='softmax'))\n",
        "    ])\n",
        "\n",
        "    # Compile the model with the provided learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_sequences,\n",
        "        train_labels_categorical,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = model.predict(test_sequences)\n",
        "    test_predictions_labels = np.argmax(test_predictions, axis=-1)\n",
        "    test_true_labels = np.argmax(test_labels_categorical, axis=-1)\n",
        "\n",
        "    # Flatten the arrays to create a single list of predictions and true labels\n",
        "    test_predictions_flat = test_predictions_labels.flatten()\n",
        "    test_true_labels_flat = test_true_labels.flatten()\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_true_labels_flat, test_predictions_flat, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"CNN Test Accuracy: {model.evaluate(test_sequences, test_labels_categorical, verbose=0)[1]:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_true_labels_flat,\n",
        "        'Predicted_Label': test_predictions_flat\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data_part4.csv',\n",
        "    test_file_path='/content/training_data_part4.csv',\n",
        "    report_file_path='CNN_part4_report.txt',\n",
        "    predictions_file_path='CNN__part4_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "snhrvrZ2c4Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7k7Q7H_TdYcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "-ShtdHPVdbhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data_for_nn(train_data, test_data):\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = [[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']]\n",
        "    test_labels = [[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']]\n",
        "\n",
        "    # Pad the labels to match the sequence length\n",
        "    train_labels = pad_sequences(train_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences(test_labels, maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels (convert to categorical format)\n",
        "    train_labels_categorical = np.where(train_labels[..., None] == -1, 0, to_categorical(train_labels, num_classes=3))\n",
        "    test_labels_categorical = np.where(test_labels[..., None] == -1, 0, to_categorical(test_labels, num_classes=3))\n",
        "\n",
        "    return train_sequences, train_labels_categorical, test_sequences, test_labels_categorical"
      ],
      "metadata": {
        "id": "VxHARvmBdezn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Broad optimization"
      ],
      "metadata": {
        "id": "n4XpDF5ldiyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def broad_optimization(train_file_path, test_file_path, report_file_path):\n",
        "    # Load and preprocess the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "\n",
        "    # Get input shape based on training data\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Broad search space\n",
        "    def build_model(hp):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=input_shape))\n",
        "        model.add(Conv1D(filters=hp.Choice('filters', [32, 64, 128, 256]),\n",
        "                         kernel_size=hp.Choice('kernel_size', [3, 5, 7]),\n",
        "                         activation='relu',\n",
        "                         padding='same'))\n",
        "        model.add(Dropout(hp.Choice('dropout_rate', [0.2, 0.3, 0.5, 0.6])))\n",
        "        model.add(TimeDistributed(Dense(hp.Choice('dense_units', [64, 128, 256]), activation='relu')))\n",
        "        model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "\n",
        "        # Compile with variable learning rate\n",
        "        model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Broad tuning using Hyperband\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_accuracy',\n",
        "        max_epochs=20,\n",
        "        directory='broad_tuning',\n",
        "        project_name='cnn_broad_optimization'\n",
        "    )\n",
        "\n",
        "    # Perform search\n",
        "    tuner.search(X_train, y_train, validation_split=0.1, epochs=10, batch_size=32)\n",
        "\n",
        "    # Retrieve best hyperparameters\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Save best hyperparameters to a report file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Best Hyperparameters from Broad Search:\\n\")\n",
        "        for param, value in best_hps.values.items():\n",
        "            f.write(f\"{param}: {value}\\n\")\n",
        "\n",
        "    print(f\"Best hyperparameters from broad search saved to {report_file_path}\")\n",
        "\n",
        "    return best_hps"
      ],
      "metadata": {
        "id": "v2irQ2VZdna0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "broad_hps = broad_optimization(\n",
        "    train_file_path='/content/training_data_part4.csv',\n",
        "    test_file_path='/content/test_data_part4.csv',\n",
        "    report_file_path='broad_optimization_report_CNN_part4.txt'\n",
        ")\n"
      ],
      "metadata": {
        "id": "8_-zTUIAdy1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine optimizion"
      ],
      "metadata": {
        "id": "W-o5nsMAdoes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tuning(train_file_path, test_file_path, broad_hps, report_file_path):\n",
        "    # Fine-tuning within narrower ranges based on broad search results\n",
        "    def build_fine_tune_model(hp):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=input_shape))\n",
        "        model.add(Conv1D(filters=hp.Int('filters', min(broad_hps['filters']-32, 96),\n",
        "                                         max(broad_hps['filters']+32, 160), step=32),\n",
        "                         kernel_size=hp.Choice('kernel_size', [5, 7, 9]),\n",
        "                         activation='relu',\n",
        "                         padding='same'))\n",
        "        model.add(Dropout(hp.Float('dropout_rate', max(0.2, broad_hps['dropout_rate']-0.1),\n",
        "                                    min(broad_hps['dropout_rate']+0.1, 0.4), step=0.05)))\n",
        "        model.add(TimeDistributed(Dense(hp.Int('dense_units', min(broad_hps['dense_units']-32, 96),\n",
        "                                               max(broad_hps['dense_units']+32, 160), step=32),\n",
        "                                      activation='relu')))\n",
        "        model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "\n",
        "        # Compile with refined learning rate\n",
        "        model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', max(1e-3, broad_hps['learning_rate']/2),\n",
        "                                                            min(broad_hps['learning_rate'], 1e-2))),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Fine-tuning with Bayesian Optimization\n",
        "    tuner = kt.BayesianOptimization(\n",
        "        build_fine_tune_model,\n",
        "        objective='val_accuracy',\n",
        "        max_trials=20,\n",
        "        directory='fine_tuning',\n",
        "        project_name='cnn_fine_tuning'\n",
        "    )\n",
        "\n",
        "    # Perform fine-tuning search\n",
        "    tuner.search(X_train, y_train, validation_split=0.1, epochs=20, batch_size=32)\n",
        "\n",
        "    # Retrieve best hyperparameters\n",
        "    best_fine_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    # Save best hyperparameters from fine-tuning to a report file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Best Hyperparameters from Fine-Tuning:\\n\")\n",
        "        for param, value in best_fine_hps.values.items():\n",
        "            f.write(f\"{param}: {value}\\n\")\n",
        "\n",
        "    print(f\"Best hyperparameters from fine-tuning saved to {report_file_path}\")\n",
        "\n",
        "    return best_fine_hps"
      ],
      "metadata": {
        "id": "MBsay_64duaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_hps = fine_tuning(\n",
        "    train_file_path='/content/training_data_part4.csv',\n",
        "    test_file_path='/content/test_data_part4.csv',\n",
        "    broad_hps={\n",
        "        'filters': 128,\n",
        "        'kernel_size': 7,\n",
        "        'dropout_rate': 0.3,\n",
        "        'dense_units': 128,\n",
        "        'learning_rate': 0.01\n",
        "    },\n",
        "    report_file_path='fine_tuning_report_part4.txt'\n",
        ")"
      ],
      "metadata": {
        "id": "LIm_1GD_d1ut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}