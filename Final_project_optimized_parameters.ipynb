{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "dBMmW6-1bUz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etwfLH52bDWu"
      },
      "outputs": [],
      "source": [
        "#As A Function\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_and_evaluate_rf(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    n_estimators=200,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=1\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # Flatten the sequences and labels\n",
        "    train_sequences_flat = train_sequences.reshape(-1, train_sequences.shape[2])\n",
        "    test_sequences_flat = test_sequences.reshape(-1, test_sequences.shape[2])\n",
        "    train_labels_flat = train_labels.flatten()\n",
        "    test_labels_flat = test_labels.flatten()\n",
        "\n",
        "    # Create mask to filter out padded positions (-1)\n",
        "    train_mask = train_labels_flat != -1\n",
        "    test_mask = test_labels_flat != -1\n",
        "\n",
        "    # Apply the mask to filter out padding\n",
        "    train_sequences_flat = train_sequences_flat[train_mask]\n",
        "    train_labels_flat = train_labels_flat[train_mask]\n",
        "    test_sequences_flat = test_sequences_flat[test_mask]\n",
        "    test_labels_flat = test_labels_flat[test_mask]\n",
        "\n",
        "    # Train the Random Forest model with the original settings\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        random_state=42\n",
        "    )\n",
        "    rf_model.fit(train_sequences_flat, train_labels_flat)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = rf_model.predict(test_sequences_flat)\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_labels_flat, test_predictions, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"Random Forest Test Accuracy: {rf_model.score(test_sequences_flat, test_labels_flat):.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_labels_flat,\n",
        "        'Predicted_Label': test_predictions\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_rf(\n",
        "    train_file_path='/content/training_data__part2_clean.csv',\n",
        "    test_file_path='/content/test_data_part2_clean.csv',\n",
        "    report_file_path='RF_opt_done_report.txt',\n",
        "    predictions_file_path='RF_opt_done_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "wpJZhOhxbZyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_rf(\n",
        "    train_file_path='/content/training_data__part3_clean.csv',\n",
        "    test_file_path='/content/test_data_part3_clean.csv',\n",
        "    report_file_path='RF_opt_done_part3_report.txt',\n",
        "    predictions_file_path='RF_opt_done_part3_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "H4zIOygIbaRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "H2a-UJVXbXjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As a function\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, TimeDistributed, Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def train_and_evaluate_cnn(\n",
        "    train_file_path,\n",
        "    test_file_path,\n",
        "    report_file_path,\n",
        "    predictions_file_path,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        "):\n",
        "    # Load the datasets\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # List of amino acids (for one-hot encoding)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "\n",
        "    # Function to one-hot encode a sequence\n",
        "    def one_hot_encode(seq, aa_list):\n",
        "        encoding = np.zeros((len(seq), len(aa_list)), dtype=int)\n",
        "        for i, aa in enumerate(seq):\n",
        "            if aa in aa_list:\n",
        "                encoding[i, aa_list.index(aa)] = 1\n",
        "        return encoding\n",
        "\n",
        "    # Encode the sequences for training and testing\n",
        "    train_encoded = [one_hot_encode(seq, amino_acids) for seq in train_data['seq']]\n",
        "    test_encoded = [one_hot_encode(seq, amino_acids) for seq in test_data['seq']]\n",
        "\n",
        "    # Find the maximum sequence length in the training and testing datasets\n",
        "    max_seq_len = max(max(len(seq) for seq in train_data['seq']),\n",
        "                      max(len(seq) for seq in test_data['seq']))\n",
        "\n",
        "    # Pad the sequences to the maximum length\n",
        "    train_sequences = pad_sequences(train_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "    test_sequences = pad_sequences(test_encoded, maxlen=max_seq_len, padding='post', dtype='float32')\n",
        "\n",
        "    # Encode the secondary structures as target labels\n",
        "    sst3_mapping = {'H': 0, 'E': 1, 'C': 2}\n",
        "    train_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in train_data['sst3']],\n",
        "                                 maxlen=max_seq_len, padding='post', value=-1)\n",
        "    test_labels = pad_sequences([[sst3_mapping[ss] for ss in sst] for sst in test_data['sst3']],\n",
        "                                maxlen=max_seq_len, padding='post', value=-1)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    train_labels_categorical = to_categorical(train_labels, num_classes=3)\n",
        "    test_labels_categorical = to_categorical(test_labels, num_classes=3)\n",
        "\n",
        "    # Define the CNN architecture\n",
        "    model = Sequential([\n",
        "        Input(shape=(train_sequences.shape[1], train_sequences.shape[2])),\n",
        "        Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
        "        Dropout(0.3),\n",
        "        Conv1D(filters=5, kernel_size=5, activation='relu', padding='same'),\n",
        "        Dropout(0.3),\n",
        "        TimeDistributed(Dense(128, activation='relu')),\n",
        "        TimeDistributed(Dense(3, activation='softmax')),\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        train_sequences,\n",
        "        train_labels_categorical,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    test_predictions = model.predict(test_sequences)\n",
        "    test_predictions_labels = np.argmax(test_predictions, axis=-1)\n",
        "    test_true_labels = np.argmax(test_labels_categorical, axis=-1)\n",
        "\n",
        "    # Flatten the arrays to create a single list of predictions and true labels\n",
        "    test_predictions_flat = test_predictions_labels.flatten()\n",
        "    test_true_labels_flat = test_true_labels.flatten()\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(test_true_labels_flat, test_predictions_flat, target_names=['H', 'E', 'C'])\n",
        "\n",
        "    # Save the classification report to a text file\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(f\"CNN Test Accuracy: {model.evaluate(test_sequences, test_labels_categorical, verbose=0)[1]:.4f}\\n\\n\")\n",
        "        f.write(\"Classification Report:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    # Save the predictions along with the true labels to a CSV file\n",
        "    results_df = pd.DataFrame({\n",
        "        'True_Label': test_true_labels_flat,\n",
        "        'Predicted_Label': test_predictions_flat\n",
        "    })\n",
        "    results_df.to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")"
      ],
      "metadata": {
        "id": "5QPPpHEfbeYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data__part2_clean.csv',\n",
        "    test_file_path='/content/test_data_part2_clean.csv',\n",
        "    report_file_path='CNN_opt_done_report.txt',\n",
        "    predictions_file_path='CNN_opt_done_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "GnTAp9uVbg_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_cnn(\n",
        "    train_file_path='/content/training_data__part3_clean.csv',\n",
        "    test_file_path='/content/test_data_part3_clean.csv',\n",
        "    report_file_path='CNN_opt_done_part3_report.txt',\n",
        "    predictions_file_path='CNN_opt_done_part3_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "-aOVL8hybjmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hybrid CNN/RNN"
      ],
      "metadata": {
        "id": "4J9bmkMabkd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#As a function\n",
        "from tensorflow.keras.layers import LSTM, TimeDistributed, Conv1D, Dropout, Dense, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def train_and_evaluate_hybrid_model(train_file_path, test_file_path, report_file_path, predictions_file_path):\n",
        "    # Step 1: Load Data\n",
        "    train_data = pd.read_csv(train_file_path)\n",
        "    test_data = pd.read_csv(test_file_path)\n",
        "\n",
        "    # Preprocess Data\n",
        "    X_train, y_train, X_test, y_test = preprocess_data_for_nn(train_data, test_data)\n",
        "\n",
        "    # Step 2: Build the Model with Best Hyperparameters\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(LSTM(units=128, return_sequences=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(64, activation='relu')))\n",
        "    model.add(TimeDistributed(Dense(3, activation='softmax')))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Step 3: Train the Model\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "    history = model.fit(X_train, y_train, epochs=10, validation_split=0.1, callbacks=[early_stopping])\n",
        "\n",
        "    # Step 4: Evaluate the Model\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "    y_true = np.argmax(y_test, axis=-1)\n",
        "\n",
        "    # Flatten predictions and true labels\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "    y_true_flat = y_true.flatten()\n",
        "\n",
        "    # Step 5: Save Report and Predictions\n",
        "    with open(report_file_path, 'w') as f:\n",
        "        f.write(\"Hybrid CNN/RNN Model Evaluation:\\n\")\n",
        "        f.write(\"\\nClassification Report:\\n\")\n",
        "        f.write(classification_report(y_true_flat, y_pred_flat, target_names=['H', 'E', 'C']))\n",
        "\n",
        "    pd.DataFrame(y_pred_flat, columns=['Predictions']).to_csv(predictions_file_path, index=False)\n",
        "\n",
        "    print(f\"Classification report saved to {report_file_path}\")\n",
        "    print(f\"Predictions saved to {predictions_file_path}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "koXrY14Hbm-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_hybrid_model(\n",
        "    train_file_path='/content/training_data__part2_clean.csv',\n",
        "    test_file_path='/content/test_data_part2_clean.csv',\n",
        "    report_file_path='Hybrid_opt_done_report.txt',\n",
        "    predictions_file_path='Hybrid_opt_done_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "CSv9N0IWbqCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate_hybrid_model(\n",
        "    train_file_path='/content/training_data__part3_clean.csv',\n",
        "    test_file_path='/content/test_data_part3_clean.csv',\n",
        "    report_file_path='Hybrid_opt_done_part3_report.txt',\n",
        "    predictions_file_path='Hybrid_opt_done_part3_predictions.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "5zj3dalxbrNk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}